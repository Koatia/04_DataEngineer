{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Установим соединение со SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/10/17 08:23:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_json, col\n",
    "from pyspark.sql.types import *\n",
    "from os.path import abspath\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"pyspark-notebook\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -sSLo data.csv 'https://davidmegginson.github.io/ourairports-data/airports.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(path=\"data.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80864"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id='6523', ident='00A', type='heliport', name='Total RF Heliport', latitude_deg='40.070985', longitude_deg='-74.933689', elevation_ft='11', continent='NA', iso_country='US', iso_region='US-PA', municipality='Bensalem', scheduled_service='no', gps_code='K00A', iata_code=None, local_code='00A', home_link='https://www.penndot.pa.gov/TravelInPA/airports-pa/Pages/Total-RF-Heliport.aspx', wikipedia_link=None, keywords=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обработка данных с использованием pyspark.sql.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.select([\"id\", \"name\", \"municipality\", \"iso_country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id='6523', name='Total RF Heliport', municipality='Bensalem', iso_country='US')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_description = df1.filter(df1['iso_country'] == 'US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.join(other=cols_description, on=['iso_country'], how='left_anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumnRenamed(\"id1\", 'c1').withColumnRenamed(\"name\", \"c2\").withColumnRenamed(\"municipality\",\n",
    "                                                                                           \"c3\").withColumnRenamed(\n",
    "    \"iso_country\", \"c4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Сохраняем данные в таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.write.saveAsTable('hive_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Получаем данные из БД sql-запросом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = spark.sql(\"\"\"\n",
    "select * from hive_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+----------------+\n",
      "| c4|    id|                  c2|              c3|\n",
      "+---+------+--------------------+----------------+\n",
      "| PR|  6680|     Cuylers Airport|       Vega Baja|\n",
      "| MH|  4650|      Utirik Airport|   Utirik Island|\n",
      "| PR|349103|Advanced Public H...|         Isabela|\n",
      "| MP|  4777|    Dynasty Heliport|San Jose, Tinian|\n",
      "| PR|  7793|Villamil-304 Ponc...|        San Juan|\n",
      "| PR|330679|Emp. Coco Beach G...|      Rio Grande|\n",
      "| PR|  8626|San Patricio Heli...|        Guaynabo|\n",
      "| PR|  9222|La Concepcion Hos...|      San German|\n",
      "| PR| 46121|Caribbean Constr ...|        Guaynabo|\n",
      "| PR|346082|PR Police-Ponce A...|           Ponce|\n",
      "+---+------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ndf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

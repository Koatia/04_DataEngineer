{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Условие:\n",
    "- Загрузите датасет по ценам на жилье Airbnb, доступный на kaggle.com: https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data\n",
    "- Подсчитайте среднее значение и дисперсию по признаку ”price” в hive. Используя Python, реализуйте скрипт mapper.py и reducer.py для расчета\n",
    "- Проверьте правильность подсчета статистики методом mapreduce в сравнении со hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Установим соединение со SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/10/21 07:51:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Настройка Spark для работы с Hive\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"pyspark-notebook\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Загрузим датасет по ценам на жилье Airbnb, доступный на kaggle.com из CSV файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- host_id: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: string (nullable = true)\n",
      " |-- number_of_reviews: string (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- reviews_per_month: string (nullable = true)\n",
      " |-- calculated_host_listings_count: string (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Чтение данных с помощью Spark в DataFrame\n",
    "data = spark.read.csv(path=\"AB_NYC_2019.csv\", header=True, inferSchema=True)\n",
    "# Отобразим структуру данных для проверки\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------+-----------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+\n",
      "|  id|                name|host_id|  host_name|neighbourhood_group|neighbourhood|latitude|longitude|      room_type|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|\n",
      "+----+--------------------+-------+-----------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+\n",
      "|2539|Clean & quiet apt...|   2787|       John|           Brooklyn|   Kensington|40.64749|-73.97237|   Private room|  149|             1|                9| 2018-10-19|             0.21|                             6|             365|\n",
      "|2595|Skylit Midtown Ca...|   2845|   Jennifer|          Manhattan|      Midtown|40.75362|-73.98377|Entire home/apt|  225|             1|               45| 2019-05-21|             0.38|                             2|             355|\n",
      "|3647|THE VILLAGE OF HA...|   4632|  Elisabeth|          Manhattan|       Harlem|40.80902| -73.9419|   Private room|  150|             3|                0|       null|             null|                             1|             365|\n",
      "|3831|Cozy Entire Floor...|   4869|LisaRoxanne|           Brooklyn| Clinton Hill|40.68514|-73.95976|Entire home/apt|   89|             1|              270| 2019-07-05|             4.64|                             1|             194|\n",
      "|5022|Entire Apt: Spaci...|   7192|      Laura|          Manhattan|  East Harlem|40.79851|-73.94399|Entire home/apt|   80|            10|                9| 2018-11-19|             0.10|                             1|               0|\n",
      "+----+--------------------+-------+-----------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проверка данных\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Подсчитаем среднее значение и дисперсию по признаку ”price”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. С использованием pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|        mean_price|    variance_price|\n",
      "+------------------+------------------+\n",
      "|152.26648115562466|56909.864820032206|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Расчет среднего значения и дисперсии по признаку ”price” в spark.\n",
    "\n",
    "from pyspark.sql.functions import mean, variance\n",
    "\n",
    "# Выбор нужного столбца \"price\" для расчета статистики\n",
    "price_data = data.select(\"price\").filter(data.price > 0)\n",
    "\n",
    "# Вычисление среднего значения и дисперсии\n",
    "price_stats = price_data.agg(\n",
    "    mean(\"price\").alias(\"mean_price\"),\n",
    "    variance(\"price\").alias(\"variance_price\")\n",
    ")\n",
    "\n",
    "# Вывод результата\n",
    "price_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. С использованием функций mapper и reducer, реализованных в python:\n",
    "\n",
    "- Mapper: Парсит строки из CSV (мы используем библиотеку csv для этого) и выбирает поле price. Возвращает кортежи с ключом \"price\", значением и счётчиком.\n",
    "- Reducer: Суммирует значения для расчета среднего и дисперсии. Мы сначала собираем результаты из RDD (mapped_rdd.collect()), а затем применяем reducer для окончательного вычисления."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T19:27:35.507711Z",
     "start_time": "2024-10-20T19:27:35.415592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя цена: 152.26648115562466\n",
      "Дисперсия цены: 56908.7003999965\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# Фильтрация строк с положительным значением 'price' и преобразование в RDD\n",
    "# rdd_data = data.select(\"price\").rdd.filter(lambda row: row.price is not None and row.price > 0)\n",
    "# Преобразуем столбец 'price' в тип float и фильтруем строки, где 'price' больше 0\n",
    "rdd_data = data.select(data[\"price\"].cast(\"float\")).rdd.filter(lambda row: row.price is not None and row.price > 0)\n",
    "\n",
    "\n",
    "# Mapper функция, которая возвращает данные в формате (ключ, (значение, count, квадрат значения))\n",
    "def mapper(row):\n",
    "    price = row.price\n",
    "    return (\"price\", (price, 1, price ** 2))\n",
    "\n",
    "\n",
    "# Reducer функция для суммирования значений\n",
    "def reducer(accum, new_val):\n",
    "    # accum = (сумма цен, количество, сумма квадратов)\n",
    "    total_price = accum[0] + new_val[0]\n",
    "    total_count = accum[1] + new_val[1]\n",
    "    sum_of_squares = accum[2] + new_val[2]\n",
    "    return (total_price, total_count, sum_of_squares)\n",
    "\n",
    "\n",
    "# Применение mapper\n",
    "mapped_rdd = rdd_data.map(mapper)\n",
    "\n",
    "# Агрегация результатов с помощью reduceByKey\n",
    "reduced_rdd = mapped_rdd.reduceByKey(reducer)\n",
    "\n",
    "\n",
    "# Применение финальной обработки для вычисления среднего и дисперсии\n",
    "def final_calculation(data):\n",
    "    total_price, total_count, sum_of_squares = data[1]\n",
    "    mean_price = total_price / total_count\n",
    "    variance = (sum_of_squares / total_count) - (mean_price ** 2)\n",
    "    return {\"mean_price\": mean_price, \"variance\": variance}\n",
    "\n",
    "\n",
    "# Собираем результаты\n",
    "final_result = reduced_rdd.map(final_calculation).collect()\n",
    "\n",
    "# Вывод результатов\n",
    "print(f\"Средняя цена: {final_result[0]['mean_price']}\")\n",
    "print(f\"Дисперсия цены: {final_result[0]['variance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. С использованием SQL-запроса (чтение данных из Hive):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняем данные в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Удаление таблицы, если она существует и сохранение данных в таблицу ab_nyc_2019\n",
    "spark.sql(\"DROP TABLE IF EXISTS ab_nyc_2019\")\n",
    "\n",
    "data.write.saveAsTable('ab_nyc_2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Получаем данные из БД sql-запросом\n",
    "\n",
    "CAST(price AS DOUBLE) >= 0 — Это условие гарантирует, что будут учитываться только строки, где price является числовым значением и больше либо равно нулю.\n",
    "Другие условия:\n",
    "price IS NOT NULL — Условие для исключения строк, где price пустой.\n",
    "price != '' — Условие для исключения пустых строк.\n",
    "price RLIKE '^[0-9]+$' — Используем регулярное выражение для фильтрации строк, содержащих только цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+------------------+-----------------+\n",
      "|total_rows|valid_prices|total_price|         avg_price|   variance_price|\n",
      "+----------+------------+-----------+------------------+-----------------+\n",
      "|     49079|       48874|  7441872.0|152.26648115562466|56908.70039999702|\n",
      "+----------+------------+-----------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Запрос в Spark SQL, который фильтрует строки с числовыми значениями в поле price\n",
    "# для проверки корректных данных и вычисления средней цены и дисперсии\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS total_rows, \n",
    "    COUNT(CASE WHEN CAST(price AS DOUBLE) > 0 THEN 1 END) AS valid_prices, \n",
    "    SUM(CASE WHEN (price IS NOT NULL AND price != '' AND price RLIKE '^[0-9]+$' AND CAST(price AS DOUBLE) > 0) \n",
    "             THEN CAST(price AS DOUBLE) \n",
    "             END) AS total_price,\n",
    "    AVG(CASE WHEN CAST(price AS DOUBLE) > 0 THEN CAST(price AS DOUBLE) END) AS avg_price,\n",
    "    -- Дисперсия: (среднее квадратичное отклонение минус квадрат среднего значения)\n",
    "    (SUM(CASE WHEN CAST(price AS DOUBLE) > 0 THEN (CAST(price AS DOUBLE) - \n",
    "    (SELECT AVG(CAST(price AS DOUBLE)) FROM ab_nyc_2019 WHERE CAST(price AS DOUBLE) > 0)) * \n",
    "    (CAST(price AS DOUBLE) - \n",
    "    (SELECT AVG(CAST(price AS DOUBLE)) FROM ab_nyc_2019 WHERE CAST(price AS DOUBLE) > 0)) END) / \n",
    "    COUNT(CASE WHEN CAST(price AS DOUBLE) > 0 THEN 1 END)) AS variance_price\n",
    "FROM ab_nyc_2019;\n",
    "\"\"\"\n",
    "\n",
    "hive_df = spark.sql(query)\n",
    "hive_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
